{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -a \"Ruiyu Hu\" -d -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pdf file\n",
    "import PyPDF2 \n",
    "\n",
    "# tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create inverse index\n",
    "from collections import defaultdict\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(text):\n",
    "    #porter = nltk.PorterStemmer()\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = text.lower() # case-folding (of the whole text string)\n",
    "    tokens = word_tokenize(tokens) # default tokenizer\n",
    "    tokens = [w for w in tokens if w not in stopwords.words('english')] # filter English stopwords\n",
    "    #tokens = [w for w in tokens if len(w) > 2]\n",
    "    #tokens = [porter.stem(tok) for tok in tokens] # apply stemmer\n",
    "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "    tokens = [w for w in tokens if w.isalpha()] # filter tokens that contain non-alphabetic character(s)\n",
    "    return tokens\n",
    "\n",
    "def tokenize(path):\n",
    "    # open PDF\n",
    "    pdf = PyPDF2.PdfFileReader(open(str(path),\"rb\"))\n",
    "    stopword_list = list(stopwords.words(\"english\"))\n",
    "\n",
    "    # read PDF file in a list\n",
    "    pdf_content = []\n",
    "    for page in pdf.pages:\n",
    "        pdf_content.append(page.extractText())\n",
    "    \n",
    "    # create a list of token\n",
    "    tokens = [None] * len(pdf_content)\n",
    "    for i in range(len(pdf_content)):\n",
    "        tokens[i] = clean_token(pdf_content[i])\n",
    "    tokens = [t for tok in tokens for t in tok] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst = tokenize('../data/documents/resume test.pdf')\n",
    "#set(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names():\n",
    "    files = []\n",
    "    #'../data/solarhrm*.pdf'\n",
    "    for file in glob.glob(\"../data/documents/*.pdf\"):\n",
    "        files.append(file)\n",
    "    return files\n",
    "\n",
    "def make_index(tokens, document_name, index, length):\n",
    "    for term in set(tokens):\n",
    "        index[term].append([document_name,tokens.count(term)])\n",
    "        length[document_name] = len(set(tokens))\n",
    "\n",
    "def write(inverted_index,length_index):\n",
    "    inv_index_file = open(\"../data/indexes/inverted_index.json\",\"w\")\n",
    "    json.dump(inverted_index,inv_index_file)\n",
    "\n",
    "    length_index_file = open(\"../data/indexes/length_index.json\",\"w\")\n",
    "    json.dump(length_index,length_index_file)\n",
    "    \n",
    "def generator():\n",
    "    resume_files = get_file_names()\n",
    "    inverted_index = defaultdict(list)\n",
    "    length_index = defaultdict(list)\n",
    "    for file in resume_files:\n",
    "        make_index(tokenize(file), file, inverted_index, length_index)\n",
    "    write(inverted_index,length_index)\n",
    "    print (\"Indexes generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create retrieval-The BM25 Weighting Scheme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "'''\n",
    "IR Book: 11.4.3\n",
    "Fomula: 11.33\n",
    "'''\n",
    "'''\n",
    "typical TREC value\n",
    "f1 = 1.2\n",
    "k2 varies from 0 to 1000\n",
    "b = 0.75\n",
    "'''\n",
    "\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "k2 = 100\n",
    "R = 0 # (set to 0 if no relevancy info is known)\n",
    "\n",
    "# MAIN METHOD\n",
    "\n",
    "def BM25(docLen, avDocLen, n, N, f, q, r):\n",
    "    p1 = ((k2 + 1) * q) / (k2 + q)\n",
    "    p2 = ((k1 + 1) * f) / (getK(docLen, avDocLen) + f)\n",
    "    p3 = log(((r + 0.5)/(R-r+0.5)) / ((n - r + 0.5)/(N - n - R + r + 0.5)))\n",
    "    return p1 * p2 * p3\n",
    "\n",
    "def getK(docLen, avDocLen):\n",
    "    return k1 * ((1 - b) + b * (float(docLen) / float(avDocLen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create Ranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average document length\n",
    "def get_avdl(length_index):\n",
    "    corpus_length = 0\n",
    "    for document in length_index:\n",
    "        corpus_length += length_index[document]\n",
    "    return float(corpus_length) / float(len(length_index))\n",
    "\n",
    "def search(query):\n",
    "    inv_index_file = open(\"../data/indexes/inverted_index.json\",\"r\")\n",
    "    inverted_index = json.load(inv_index_file)\n",
    "\n",
    "    length_index_file = open(\"../data/indexes/length_index.json\",\"r\")\n",
    "    length_index = json.load(length_index_file)\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    #query_tokens = query.split()\n",
    "    #for token in query_tokens:\n",
    "    for token in query:\n",
    "        if token in inverted_index.keys():\n",
    "            for entry in inverted_index[token]:\n",
    "                scores[entry[0]] = BM25(length_index[entry[0]],get_avdl(length_index),len(inverted_index[token]),len(length_index),entry[1],1,0)\n",
    "    return sorted(scores.items(),key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(keyword):\n",
    "    results = search(keyword)\n",
    "    \n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword = 'machine learning'\n",
    "matching('i like python and sql ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jd side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = tokenize('../data/job_description/qualification.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up env locally\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:\\Users\\RayHu\\ruiyu-gcp-4ac10836d3b1.json' \n",
    "\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = language.LanguageServiceClient()\n",
    "# part-of-speech tags from list(enums.PartOfSpeech.Tag)\n",
    "pos_tag = ('UNKNOWN', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM',\n",
    "           'PRON', 'PRT', 'PUNCT', 'VERB', 'X', 'AFFIX')\n",
    "tags = ['NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean =[]\n",
    "output = []\n",
    "for _ in lst:\n",
    "    #output =[]\n",
    "    #doc = ' '.join(_)\n",
    "    document = language.types.Document(content = _, type=enums.Document.Type.PLAIN_TEXT)\n",
    "    tokens = client.analyze_syntax(document).tokens\n",
    "    for token in tokens:\n",
    "        if pos_tag[token.part_of_speech.tag] in tags:\n",
    "            output.append(token.text.content)\n",
    "    #clean.append([k for k in output]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(output)\n",
    "query = [key for key, val in c.most_common(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
