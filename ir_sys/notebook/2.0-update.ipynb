{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -a \"Ruiyu Hu\" -d -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pdf file\n",
    "import PyPDF2 \n",
    "\n",
    "# tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create inverse index\n",
    "from collections import defaultdict\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "#for google api \n",
    "# set up env locally\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:\\Users\\RayHu\\ruiyu-gcp-4ac10836d3b1.json' \n",
    "\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(text):\n",
    "    #porter = nltk.PorterStemmer()\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = text.lower() # case-folding (of the whole text string)\n",
    "    tokens = word_tokenize(tokens) # default tokenizer\n",
    "    tokens = [w for w in tokens if w not in stopwords.words('english')] # filter English stopwords\n",
    "    #tokens = [w for w in tokens if len(w) > 2]\n",
    "    #tokens = [porter.stem(tok) for tok in tokens] # apply stemmer\n",
    "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "    tokens = [w for w in tokens if w.isalpha()] # filter tokens that contain non-alphabetic character(s)\n",
    "    return tokens\n",
    "\n",
    "def tokenize(path):\n",
    "    # open PDF\n",
    "    pdf = PyPDF2.PdfFileReader(open(str(path),\"rb\"))\n",
    "    #stopword_list = list(stopwords.words(\"english\"))\n",
    "\n",
    "    #read PDF file in a list\n",
    "    pdf_content = []\n",
    "    for page in pdf.pages:\n",
    "        pdf_content.append(page.extractText())\n",
    "    \n",
    "    # perform noun phrases\n",
    "    noun_lst = []\n",
    "    for _ in pdf_content:\n",
    "        for word, tag in (TextBlob(_).tags):\n",
    "            if tag in (\"NN\", \"NNS\", \"NNP\", \"NNPS\"):\n",
    "                noun_lst.append(word)\n",
    "        \n",
    "    # create a list of token\n",
    "\n",
    "    \n",
    "    for token in noun_lst:\n",
    "        token = token.lower() \n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PyPDF2.PdfFileReader(open(str('../data/documents/ml001.pdf'),\"rb\"))\n",
    "    #stopword_list = list(stopwords.words(\"english\"))\n",
    "\n",
    "    #read PDF file in a list\n",
    "pdf_content = []\n",
    "for page in pdf.pages:\n",
    "    pdf_content.append(page.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for _ in pdf_content:\n",
    "    for word, tag in (TextBlob(_).tags):\n",
    "        if tag in (\"NN\", \"NNS\", \"NNP\", \"NNPS\"):\n",
    "            lst.append(word)\n",
    "tokens = [w for w in lst if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine', 'Learning', 'Engineer', 'Controls', 'Engineer', 'Data', 'Scientist', 'Berkeley', 'CA', 'Work', 'Experience', 'Machine', 'Learning', 'Engineer', 'Nafith', 'Logistics', 'Amman', 'JO', 'June', 'January', 'layer', 'security', 'ground', 'transportation', 'truck', 'passage', 'camera', 'accuracy', 'data', 'net', 'scratch', 'doublet', 'loss', 'vector', 'embeddings', 'tensorflow', 'Python', 'Instructor', 'Python', 'Summer', 'Camp', 'Amman', 'June', 'August', 'week', 'python', 'course', 'f', 'beginners', 'coding', 'jupyter', 'notebooks', 'environment', 'group', 'people', 'student', 'course', 'deliverable', 'project', 'email', 'sender', 'course', 'media', 'relationships', 'networking', 'Researcher', 'COMPLEXITY', 'GROUP', 'May', 'May', 'learning', 'data', 'analysis', 'detection', 'Explored', 'mu', 'versions', 'clustering', 'clustering', 'distance', 'metrics', 'pipeline', 'testing', 'multip', 'machine', 'learning', 'models', 'acceleration', 'sensors', 'sensors', 'budget', 'framework', 'Education', 'Mechanical', 'Engineering', 'UC', 'BERKELEY', 'August', 'May', 'Mechanical', 'Engineering', 'UC', 'BERKELEY', 'August', 'May', 'Skills', 'Algorithm', 'Less', 'year', 'Machine', 'Learning', 'Less', 'year', 'Java', 'Less', 'year', 'Matlab', 'Less', 'year', 'Computer', 'Science', 'Less', 'year', 'Python', 'years', 'Less', 'year', 'Data', 'Mining', 'Less', 'year', 'Data', 'Analysis', 'Less', 'year', 'Sol', 'year', 'SQL', 'Less', 'year', 'Statistics', 'year', 'Linux', 'Less', 'year']"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-527-ebf4c75ec280>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-527-ebf4c75ec280>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    tok = [tok if tok.isalpha()]\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for tok in lst:\n",
    "    tok = tok.lower()\n",
    "    tok = lemmatizer.lemmatize(tok) \n",
    "    tok = [tok if tok.isalpha()]\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine', 'learning', 'engineer', 'control', 'engineer', 'data', 'scientist', 'berkeley', 'ca', 'work', 'experience', 'machine', 'learning', 'engineer', 'logistics', 'amman', 'jo', 'june', 'january', 'layer', 'security', 'ground', 'transportation', 'truck', 'passage', 'camera', 'accuracy', 'data', 'writing', 'scratch', 'doublet', 'loss', 'vector', 'python', 'instructor', 'python', 'summer', 'camp', 'amman', 'june', 'week', 'python', 'course', 'f', 'beginner', 'teaching', 'start', 'coding', 'notebook', 'environment', 'group', 'people', 'work', 'student', 'course', 'project', 'email', 'sender', 'course', 'relationship', 'networking', 'researcher', 'complexity', 'group', 'learning', 'data', 'analysis', 'anomaly', 'detection', 'mu', 'version', 'clustering', 'clustering', 'distance', 'pipeline', 'testing', 'multip', 'machine', 'learning', 'model', 'acceleration', 'sensor', 'sensor', 'budget', 'framework', 'education', 'engineering', 'uc', 'berkeley', 'engineering', 'uc', 'berkeley', 'skill', 'algorithm', 'year', 'machine', 'learning', 'year', 'java', 'year', 'year', 'computer', 'science', 'year', 'python', 'year', 'year', 'data', 'mining', 'year', 'data', 'analysis', 'year', 'sol', 'year', 'sql', 'year', 'year', 'linux', 'year']"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tokenize('../data/documents/ml001.pdf')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names():\n",
    "    files = []\n",
    "    #'../data/solarhrm*.pdf'\n",
    "    for file in glob.glob(\"../data/documents/*.pdf\"):\n",
    "    #for file in glob.glob('../data/documents/*.txt'):\n",
    "        files.append(file)\n",
    "    return files\n",
    "\n",
    "def make_idx(tokens, doc_name, idx, length):\n",
    "    for term in set(tokens):\n",
    "        idx[term].append([doc_name,tokens.count(term)])\n",
    "        len[doc_name] = len(set(tokens))\n",
    "\n",
    "def write(inverted_idx,len_idx):\n",
    "    inv_idx_file = open(\"../data/indexes/inverted_idx.json\",\"w\")\n",
    "    json.dump(inverted_idx,inv_idx_file)\n",
    "\n",
    "    len_idx_file = open(\"../data/indexes/len_idx.json\",\"w\")\n",
    "    json.dump(len_idx,len_idx_file)\n",
    "    \n",
    "def generate_idx():\n",
    "    resume_files = get_file_names()\n",
    "    inverted_index = defaultdict(list)\n",
    "    length_index = defaultdict(list)\n",
    "    for file in resume_files:\n",
    "        make_index(tokenize(file), file, inverted_index, length_index)\n",
    "    write(inverted_index,length_index)\n",
    "    print (\"Indexes generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes generated\n"
     ]
    }
   ],
   "source": [
    "generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create retrieval-The BM25 Weighting Scheme**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formula**\n",
    "\n",
    "\n",
    "For query Q and document d, we have BM25 d of Q:\n",
    "\n",
    "$ score(D,Q)=\\frac{(k_2+1)qf_i}{k_2+qf_i}\\times \\frac{(k_1+1)\\times f_i}{f_i+K}\\times ln(\\frac{(r_i+0.5)/(R-r_i+0.5)}{(n_i-r_i+0.5)/(N-n_i-R+r_i+0.5)}+1)$\n",
    "\n",
    "> Reference: Elasticsearch and IR Text Book Fomula 11.33\n",
    "\n",
    "* $r_i$ is the # of relevant documents containing term i \n",
    "* $n_i$  is the # of docs containing term i\n",
    "* $N$ is the total # of docs in the collection\n",
    "* $R$ is the number of relevant documents for this query  (set to 0 if no relevancy info is known)\n",
    "* $f_i$  is the frequency of term i in the doc under consideration\n",
    "* $qf_i$ is the frequency of term i in the query\n",
    "* $k_1$ determines how the tf component of the term weight changes as $f_i$\n",
    "  increases. (if 0, then tf component is ignored.) \n",
    "* $k_2$ Typical values make the equation less sensitive to k2 than k1 because query term frequencies are much lower and less variable than doc term frequencies.\n",
    "* $K$ It equals to ($k_1(1-b+b\\times l_d/avg\\_l)$). Its role is basically to normalize the tf component by document length.\n",
    "* $b$ regulates the impact of length normalization. (0 means none; 1 is full normalization.) \n",
    "* $ln$: if $n_i$ > $\\frac{N}{2}$, the result of $ln$ could be negative. Therefore, we plus 1 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "'''\n",
    "IR Book: 11.4.3\n",
    "Fomula: 11.33\n",
    "'''\n",
    "'''\n",
    "typical TREC value (Text Retrieval Conference (TREC).)\n",
    "f1 = 1.2\n",
    "k2 varies from 0 to 1000\n",
    "b = 0.75\n",
    "'''\n",
    "\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "k2 = 100\n",
    "R = 0 # (set it to 0 since no relevancy info is known)\n",
    "\n",
    "\n",
    "\n",
    "def BM25(doc_len, avg_doc_len, n_doc_w_term, n_total_doc, freq_term_doc, freq_term_query, rel_doc_w_term):\n",
    "    n = n_doc_w_term\n",
    "    N = n_total_doc\n",
    "    f = freq_term_doc\n",
    "    q = freq_term_query\n",
    "    r = rel_doc_w_term\n",
    "    p1 = ((k2 + 1) * q) / (k2 + q) #Relevance between term and query\n",
    "    p2 = ((k1 + 1) * f) / (getK(doc_len, avg_doc_len) + f) #Relevance between term and document\n",
    "    p3 = log((((r + 0.5)/(R-r+0.5)) / ((n - r + 0.5)/(N - n - R + r + 0.5)))+1) # Term Weight\n",
    "    return p1 * p2 * p3\n",
    "\n",
    "def getK(doc_len, avg_doc_len):\n",
    "    return k1 * ((1 - b) + b * (float(doc_len) / float(avg_doc_len)))\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create Ranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average document length\n",
    "def get_avg_doc_len(len_idx):\n",
    "    _length = 0\n",
    "    for doc in len_idx:\n",
    "        _length += len_idx[doc]\n",
    "    return float(_length) / float(len(len_idx))\n",
    "\n",
    "def search(query):\n",
    "    inv_idx_file = open(\"../data/indexes/inverted_idx.json\",\"r\")\n",
    "    inverted_idx = json.load(inv_idx_file)\n",
    "\n",
    "    len_idx_file = open(\"../data/indexes/len_idx.json\",\"r\")\n",
    "    len_idx = json.load(len_idx_file)\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    query_tokens = query.split()\n",
    "    for token in query_tokens:\n",
    "    #for token in query:\n",
    "        if token in inverted_idx.keys():\n",
    "            for entry in inverted_idx[token]:\n",
    "                bm25_val = BM25(len_idx[entry[0]],get_avg_doc_len(len_idx),len(inverted_idx[token]),len(len_idx),entry[1],1,0)\n",
    "                scores[entry[0]] = round(10* sigmoid(bm25_val)-5,4)\n",
    "    result = sorted(scores.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    #result = sorted(norm_scores.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(keyword):\n",
    "    #generator()\n",
    "    results = search(keyword)\n",
    "    \n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('../data/documents\\\\da001.pdf', 0.0602)\n",
      "('../data/documents\\\\da002.pdf', 0.0602)\n",
      "('../data/documents\\\\da003.pdf', 0.0602)\n",
      "('../data/documents\\\\da004.pdf', 0.0602)\n",
      "('../data/documents\\\\da005.pdf', 0.0602)\n",
      "('../data/documents\\\\de001.pdf', 0.0602)\n",
      "('../data/documents\\\\de002.pdf', 0.0602)\n",
      "('../data/documents\\\\de003.pdf', 0.0602)\n",
      "('../data/documents\\\\de004.pdf', 0.0602)\n",
      "('../data/documents\\\\de005.pdf', 0.0602)\n",
      "('../data/documents\\\\ds001.pdf', 0.0602)\n",
      "('../data/documents\\\\ds002.pdf', 0.0602)\n",
      "('../data/documents\\\\ds003.pdf', 0.0602)\n",
      "('../data/documents\\\\ds004.pdf', 0.0602)\n",
      "('../data/documents\\\\ds005.pdf', 0.0602)\n",
      "('../data/documents\\\\ml001.pdf', 0.0602)\n",
      "('../data/documents\\\\ml002.pdf', 0.0602)\n",
      "('../data/documents\\\\ml003.pdf', 0.0602)\n",
      "('../data/documents\\\\ml004.pdf', 0.0602)\n",
      "('../data/documents\\\\ml005.pdf', 0.0602)\n"
     ]
    }
   ],
   "source": [
    "matching('data scientist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jd side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jd_1(dirt):\n",
    "    '''\n",
    "    Method 1: only keep noun in the job description\n",
    "    '''\n",
    "    lst = tokenize(dirt)\n",
    "    \n",
    "    client = language.LanguageServiceClient()\n",
    "    # part-of-speech tags from list(enums.PartOfSpeech.Tag)\n",
    "    pos_tag = ('UNKNOWN', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM',\n",
    "               'PRON', 'PRT', 'PUNCT', 'VERB', 'X', 'AFFIX')\n",
    "    tags = ['NOUN']\n",
    "    \n",
    "    output = []\n",
    "    for _ in lst:\n",
    "        #output =[]\n",
    "        #doc = ' '.join(_)\n",
    "        document = language.types.Document(content = _, type=enums.Document.Type.PLAIN_TEXT)\n",
    "        tokens = client.analyze_syntax(document).tokens\n",
    "        for token in tokens:\n",
    "            if pos_tag[token.part_of_speech.tag] in tags:\n",
    "                output.append(token.text.content)\n",
    "                \n",
    "    c = Counter(output)\n",
    "    query = [key for key, val in c.most_common(20)]\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jd_2(dirt):\n",
    "    '''\n",
    "    method 2: terms sorted by tf idf weights\n",
    "    '''\n",
    "    doc = tokenize(dirt)\n",
    "    \n",
    "    cv=CountVectorizer(stop_words=stopwords.words('english'))\n",
    "    word_count_vector=cv.fit_transform(doc)\n",
    "    \n",
    "    #calculate the weights for each term in each document\n",
    "    tfidf_transformer=TfidfTransformer()\n",
    "    tf_idf_vector = tfidf_transformer.fit_transform(word_count_vector)\n",
    "    #the top 20 terms by average tf-idf weight\n",
    "    weights = np.asarray(tf_idf_vector.mean(axis=0)).ravel().tolist()\n",
    "    weights_df = pd.DataFrame({'term': cv.get_feature_names(), 'weight': weights}).sort_values(by='weight', ascending=False).head(20)\n",
    "    return weights_df.term.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirt = '../data/job_description/ml-jd-adobe.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "#search(clean_jd_1(dirt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('../data/documents\\\\da002.pdf', 1.9554483101714493), ('../data/documents\\\\ds002.pdf', 1.939375031370122), ('../data/documents\\\\ml001.pdf', 1.5029566680437219), ('../data/documents\\\\de005.pdf', 1.170008090489953), ('../data/documents\\\\de001.pdf', 0.0), ('../data/documents\\\\de004.pdf', 0.0), ('../data/documents\\\\ds001.pdf', 0.0), ('../data/documents\\\\ds003.pdf', 0.0), ('../data/documents\\\\ds004.pdf', 0.0), ('../data/documents\\\\ml002.pdf', 0.0), ('../data/documents\\\\da003.pdf', 0.0), ('../data/documents\\\\da001.pdf', 0.0), ('../data/documents\\\\ml004.pdf', 0.0), ('../data/documents\\\\ds005.pdf', -0.17746847325794762), ('../data/documents\\\\da004.pdf', -0.18782871493816702), ('../data/documents\\\\ml003.pdf', -0.2026139425178263), ('../data/documents\\\\de003.pdf', -0.2162306887947264), ('../data/documents\\\\de002.pdf', -0.2892254673064674), ('../data/documents\\\\da005.pdf', -0.33071276322252563), ('../data/documents\\\\ml005.pdf', -2.9816561435328213)]\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "print(search(clean_jd_2(dirt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## approach 2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd = pd.read_json('../data/jd/data_scientist.json').T\n",
    "jd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(jd.posting[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd.posting[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(jd.posting[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(jd.posting[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TextBlob(\"Python is a high-level, general-purpose machine learning language.\")\n",
    "test.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
