{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruiyu Hu 2019-03-02 \n",
      "\n",
      "CPython 3.6.7\n",
      "IPython 7.0.1\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%watermark -a \"Ruiyu Hu\" -d -v -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pdf file\n",
    "import PyPDF2 \n",
    "\n",
    "# tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create inverse index\n",
    "from collections import defaultdict\n",
    "\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(text):\n",
    "    #porter = nltk.PorterStemmer()\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = text.lower() # case-folding (of the whole text string)\n",
    "    tokens = word_tokenize(tokens) # default tokenizer\n",
    "    tokens = [w for w in tokens if w not in stopwords.words('english')] # filter English stopwords\n",
    "    #tokens = [w for w in tokens if len(w) > 2]\n",
    "    #tokens = [porter.stem(tok) for tok in tokens] # apply stemmer\n",
    "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "    tokens = [w for w in tokens if w.isalpha()] # filter tokens that contain non-alphabetic character(s)\n",
    "    return tokens\n",
    "\n",
    "def tokenize(path):\n",
    "    # open PDF\n",
    "    pdf = PyPDF2.PdfFileReader(open(str(path),\"rb\"))\n",
    "    stopword_list = list(stopwords.words(\"english\"))\n",
    "\n",
    "    # read PDF file in a list\n",
    "    pdf_content = []\n",
    "    for page in pdf.pages:\n",
    "        pdf_content.append(page.extractText())\n",
    "    \n",
    "    # create a list of token\n",
    "    tokens = [None] * len(pdf_content)\n",
    "    for i in range(len(pdf_content)):\n",
    "        tokens[i] = clean_token(pdf_content[i])\n",
    "    tokens = [t for tok in tokens for t in tok] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = tokenize('../data/documents/the resume.pdf')\n",
    "#set(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names():\n",
    "    files = []\n",
    "    #'../data/solarhrm*.pdf'\n",
    "    for file in glob.glob(\"../data/documents/*.pdf\"):\n",
    "        files.append(file)\n",
    "    return files\n",
    "\n",
    "def make_index(tokens, document_name, index, length):\n",
    "    for term in set(tokens):\n",
    "        index[term].append([document_name,tokens.count(term)])\n",
    "        length[document_name] = len(set(tokens))\n",
    "\n",
    "def write(inverted_index,length_index):\n",
    "    inv_index_file = open(\"../data/indexes/inverted_index.json\",\"w\")\n",
    "    json.dump(inverted_index,inv_index_file)\n",
    "\n",
    "    length_index_file = open(\"../data/indexes/length_index.json\",\"w\")\n",
    "    json.dump(length_index,length_index_file)\n",
    "    \n",
    "def generator():\n",
    "    resume_files = get_file_names()\n",
    "    inverted_index = defaultdict(list)\n",
    "    length_index = defaultdict(list)\n",
    "    for file in resume_files:\n",
    "        make_index(tokenize(file), file, inverted_index, length_index)\n",
    "    write(inverted_index,length_index)\n",
    "    print (\"Indexes generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create retrieval-The BM25 Weighting Scheme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "'''\n",
    "Using the following formula to calculate BM25\n",
    "((k3 + 1)q)/((k3 + q)) * ((k1 + 1)f)/((K + f)) * log((r + 0.5)(N − n − R + r + 0.5))/((n − r + 0.5)(R − r + 0.5))\n",
    "REFERENCE: https://xapian.org/docs/bm25.html\n",
    "'''\n",
    "\n",
    "# DEFINING CONSTANTS\n",
    "\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "k2 = 100\n",
    "R = 0 #Since no relevance info is available\n",
    "\n",
    "# MAIN METHOD\n",
    "\n",
    "def BM25(docLen, avDocLen, n, N, f, q, r):\n",
    "    p1 = ((k2 + 1) * q) / (k2 + q)\n",
    "    p2 = ((k1 + 1) * f) / (getK(docLen, avDocLen) + f)\n",
    "    p3 = log((r + 0.5) * (N - n - R + r + 0.5)) / ((n - r + 0.5) * (R - r + 0.5))\n",
    "    return p1 * p2 * p3\n",
    "\n",
    "def getK(docLen, avDocLen):\n",
    "    return k1 * ((1 - b) + b * (float(docLen) / float(avDocLen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create Ranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "#from retrieval import BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average document length\n",
    "def get_avdl(length_index):\n",
    "    corpus_length = 0\n",
    "    for document in length_index:\n",
    "        corpus_length += length_index[document]\n",
    "    return float(corpus_length) / float(len(length_index))\n",
    "\n",
    "def search(query):\n",
    "    inv_index_file = open(\"../data/indexes/inverted_index.json\",\"r\")\n",
    "    inverted_index = json.load(inv_index_file)\n",
    "\n",
    "    length_index_file = open(\"../data/indexes/length_index.json\",\"r\")\n",
    "    length_index = json.load(length_index_file)\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "    query_tokens = query.split()\n",
    "    for token in query_tokens:\n",
    "        for entry in inverted_index[token]:\n",
    "            scores[entry[0]] = BM25(length_index[entry[0]],get_avdl(length_index),len(inverted_index[token]),len(length_index),entry[1],1,0)\n",
    "    return sorted(scores.items(),key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Enter search query\")\n",
    "keywords = input(\":: \")\n",
    "results = search(keywords)\n",
    "print(\"\\nThe Matching Resumes Are:\")\n",
    "for result in results:\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create boolean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BooleanModel():\n",
    "    \n",
    "    @staticmethod\n",
    "    def and_operation(left_operand, right_operand):\n",
    "        # perform 'merge'\n",
    "        result = []                                 # results list to be returned\n",
    "        l_index = 0                                 # current index in left_operand\n",
    "        r_index = 0                                 # current index in right_operand\n",
    "        l_skip = int(math.sqrt(len(left_operand)))  # skip pointer distance for l_index\n",
    "        r_skip = int(math.sqrt(len(right_operand))) # skip pointer distance for r_index\n",
    "\n",
    "        while (l_index < len(left_operand) and r_index < len(right_operand)):\n",
    "            l_item = left_operand[l_index]  # current item in left_operand\n",
    "            r_item = right_operand[r_index] # current item in right_operand\n",
    "            \n",
    "            # case 1: if match\n",
    "            if (l_item == r_item):\n",
    "                result.append(l_item)   # add to results\n",
    "                l_index += 1            # advance left index\n",
    "                r_index += 1            # advance right index\n",
    "            \n",
    "            # case 2: if left item is more than right item\n",
    "            elif (l_item > r_item):\n",
    "                # if r_index can be skipped (if new r_index is still within range and resulting item is <= left item)\n",
    "                if (r_index + r_skip < len(right_operand)) and right_operand[r_index + r_skip] <= l_item:\n",
    "                    r_index += r_skip\n",
    "                # else advance r_index by 1\n",
    "                else:\n",
    "                    r_index += 1\n",
    "\n",
    "            # case 3: if left item is less than right item\n",
    "            else:\n",
    "                # if l_index can be skipped (if new l_index is still within range and resulting item is <= right item)\n",
    "                if (l_index + l_skip < len(left_operand)) and left_operand[l_index + l_skip] <= r_item:\n",
    "                    l_index += l_skip\n",
    "                # else advance l_index by 1\n",
    "                else:\n",
    "                    l_index += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def or_operation(left_operand, right_operand):\n",
    "        result = []     # union of left and right operand\n",
    "        l_index = 0     # current index in left_operand\n",
    "        r_index = 0     # current index in right_operand\n",
    "\n",
    "        # while lists have not yet been covered\n",
    "        while (l_index < len(left_operand) or r_index < len(right_operand)):\n",
    "            # if both list are not yet exhausted\n",
    "            if (l_index < len(left_operand) and r_index < len(right_operand)):\n",
    "                l_item = left_operand[l_index]  # current item in left_operand\n",
    "                r_item = right_operand[r_index] # current item in right_operand\n",
    "                \n",
    "                # case 1: if items are equal, add either one to result and advance both pointers\n",
    "                if (l_item == r_item):\n",
    "                    result.append(l_item)\n",
    "                    l_index += 1\n",
    "                    r_index += 1\n",
    "\n",
    "                # case 2: l_item greater than r_item, add r_item and advance r_index\n",
    "                elif (l_item > r_item):\n",
    "                    result.append(r_item)\n",
    "                    r_index += 1\n",
    "\n",
    "                # case 3: l_item lower than r_item, add l_item and advance l_index\n",
    "                else:\n",
    "                    result.append(l_item)\n",
    "                    l_index += 1\n",
    "\n",
    "            # if left_operand list is exhausted, append r_item and advance r_index\n",
    "            elif (l_index >= len(left_operand)):\n",
    "                r_item = right_operand[r_index]\n",
    "                result.append(r_item)\n",
    "                r_index += 1\n",
    "\n",
    "            # else if right_operand list is exhausted, append l_item and advance l_index \n",
    "            else:\n",
    "                l_item = left_operand[l_index]\n",
    "                result.append(l_item)\n",
    "                l_index += 1\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def not_operation(right_operand, indexed_docIDs):\n",
    "        # complement of an empty list is list of all indexed docIDs\n",
    "        if (not right_operand):\n",
    "            return indexed_docIDs\n",
    "        \n",
    "        result = []\n",
    "        r_index = 0 # index for right operand\n",
    "        for item in indexed_docIDs:\n",
    "            # if item do not match that in right_operand, it belongs to compliment \n",
    "            if (item != right_operand[r_index]):\n",
    "                result.append(item)\n",
    "            # else if item matches and r_index still can progress, advance it by 1\n",
    "            elif (r_index + 1 < len(right_operand)):\n",
    "                r_index += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IR system level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "\n",
    "#import BooleanModel\n",
    "\n",
    "# Build from sources: https://github.com/laurentluce/python-algorithms\n",
    "from algorithms.binary_tree import Node\n",
    "\n",
    "class IRSystem():\n",
    "\n",
    "    def __init__(self, docs=None, stop_words=[]):\n",
    "        if docs is None:\n",
    "            raise UserWarning('Docs should not be none')\n",
    "        self._docs = docs\n",
    "        self._stemmer = nltk.stem.porter.PorterStemmer()\n",
    "        self._inverted_index = self._preprocess_corpus(stop_words)\n",
    "        self._print_inverted_index()\n",
    "\n",
    "    def _preprocess_corpus(self, stop_words):\n",
    "        index = {}\n",
    "        for i, doc in enumerate(self._docs):\n",
    "            for word in doc.split():\n",
    "                if word in stop_words:\n",
    "                    continue\n",
    "                token = self._stemmer.stem(word.lower())\n",
    "                if index.get(token, -244) == -244:\n",
    "                    index[token] = Node(i + 1)\n",
    "                elif isinstance(index[token], Node):\n",
    "                    index[token].insert(i + 1)\n",
    "                else:\n",
    "                    raise UserWarning('Wrong data type for posting list')\n",
    "        return index\n",
    "\n",
    "    def _print_inverted_index(self):\n",
    "        print('INVERTED INDEX:\\n')\n",
    "        for word, tree in self._inverted_index.items():\n",
    "            print('{}: {}'.format(word, [doc_id for doc_id in tree.tree_data() if doc_id != None]))\n",
    "        print()\n",
    "\n",
    "    def _get_posting_list(self, word):\n",
    "        return [doc_id for doc_id in self._inverted_index[word].tree_data() if doc_id != None]\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_query(infix_tokens):\n",
    "        \"\"\" Parse Query \n",
    "        Parsing done using Shunting Yard Algorithm \n",
    "        \"\"\"\n",
    "        precedence = {}\n",
    "        precedence['NOT'] = 3\n",
    "        precedence['AND'] = 2\n",
    "        precedence['OR'] = 1\n",
    "        precedence['('] = 0\n",
    "        precedence[')'] = 0    \n",
    "\n",
    "        output = []\n",
    "        operator_stack = []\n",
    "\n",
    "        for token in infix_tokens:\n",
    "            if (token == '('):\n",
    "                operator_stack.append(token)\n",
    "            \n",
    "            # if right bracket, pop all operators from operator stack onto output until we hit left bracket\n",
    "            elif (token == ')'):\n",
    "                operator = operator_stack.pop()\n",
    "                while operator != '(':\n",
    "                    output.append(operator)\n",
    "                    operator = operator_stack.pop()\n",
    "            \n",
    "            # if operator, pop operators from operator stack to queue if they are of higher precedence\n",
    "            elif (token in precedence):\n",
    "                # if operator stack is not empty\n",
    "                if (operator_stack):\n",
    "                    current_operator = operator_stack[-1]\n",
    "                    while (operator_stack and precedence[current_operator] > precedence[token]):\n",
    "                        output.append(operator_stack.pop())\n",
    "                        if (operator_stack):\n",
    "                            current_operator = operator_stack[-1]\n",
    "                operator_stack.append(token) # add token to stack\n",
    "            else:\n",
    "                output.append(token.lower())\n",
    "\n",
    "        # while there are still operators on the stack, pop them into the queue\n",
    "        while (operator_stack):\n",
    "            output.append(operator_stack.pop())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def process_query(self, query):\n",
    "        # prepare query list\n",
    "        query = query.replace('(', '( ')\n",
    "        query = query.replace(')', ' )')\n",
    "        query = query.split(' ')\n",
    "\n",
    "        indexed_docIDs = list(range(1, len(self._docs) + 1))\n",
    "\n",
    "        results_stack = []\n",
    "        postfix_queue = collections.deque(self._parse_query(query)) # get query in postfix notation as a queue\n",
    "\n",
    "        while postfix_queue:\n",
    "            token = postfix_queue.popleft()\n",
    "            result = [] # the evaluated result at each stage\n",
    "            # if operand, add postings list for term to results stack\n",
    "            if (token != 'AND' and token != 'OR' and token != 'NOT'):\n",
    "                token = self._stemmer.stem(token) # stem the token\n",
    "                # default empty list if not in dictionary\n",
    "                if (token in self._inverted_index):\n",
    "                    result = self._get_posting_list(token)\n",
    "            \n",
    "            elif (token == 'AND'):\n",
    "                right_operand = results_stack.pop()\n",
    "                left_operand = results_stack.pop()\n",
    "                result = BooleanModel.and_operation(left_operand, right_operand)   # evaluate AND\n",
    "\n",
    "            elif (token == 'OR'):\n",
    "                right_operand = results_stack.pop()\n",
    "                left_operand = results_stack.pop()\n",
    "                result = BooleanModel.or_operation(left_operand, right_operand)    # evaluate OR\n",
    "\n",
    "            elif (token == 'NOT'):\n",
    "                right_operand = results_stack.pop()\n",
    "                result = BooleanModel.not_operation(right_operand, indexed_docIDs) # evaluate NOT\n",
    "\n",
    "            results_stack.append(result)                        \n",
    "\n",
    "        # NOTE: at this point results_stack should only have one item and it is the final result\n",
    "        if len(results_stack) != 1: \n",
    "            print(\"ERROR: Invalid Query. Please check query syntax.\") # check for errors\n",
    "            return None\n",
    "        \n",
    "        return results_stack.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if __name__ == '__main__':\\n    try:\\n        main()\\n    except KeyboardInterrupt as e:\\n        print('EXIT')\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import timeit\n",
    "\n",
    "#from ir_system import IRSystem\n",
    "\n",
    "docs = ['hello i m a machine learning engineer', \n",
    "        'hello bad world machine engineering people', \n",
    "        'the world is a bad place',\n",
    "        'engineering a great machine that learns']\n",
    "\n",
    "stop_words = ['is', 'a', 'for', 'the', 'of']\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Information Retrieval System Configuration')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    ir = IRSystem(docs, stop_words=stop_words)\n",
    "\n",
    "    while True:\n",
    "        query = input('Enter boolean query: ')\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        results = ir.process_query(query)\n",
    "        stop = timeit.default_timer()\n",
    "        if results is not None:\n",
    "            print ('Processing time: {:.5} secs'.format(stop - start))\n",
    "            print('\\nDoc IDS: ')\n",
    "            print(results)\n",
    "        print()\n",
    "\n",
    "'''if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt as e:\n",
    "        print('EXIT')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\RayHu\\AppData\\Roaming\\jupyter\\runtime\\kernel-16975cab-fd67-4b3d-b43d-1ee3cba5675c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-7dcb9b1c3740>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIRSystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-7dcb9b1c3740>\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Information Retrieval System Configuration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unrecognized arguments: %s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1737\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1738\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2391\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2392\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'prog'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2393\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36mexit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2378\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2379\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2380\u001b[1;33m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
