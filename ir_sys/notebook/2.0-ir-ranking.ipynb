{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -a \"Ruiyu Hu\" -d -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pdf file\n",
    "import PyPDF2 \n",
    "\n",
    "# tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create inverse index\n",
    "from collections import defaultdict\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "#for google api \n",
    "# set up env locally\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:\\Users\\RayHu\\ruiyu-gcp-4ac10836d3b1.json' \n",
    "\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "\n",
    "\n",
    "import warning\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(text):\n",
    "    #porter = nltk.PorterStemmer()\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = text.lower() # case-folding (of the whole text string)\n",
    "    tokens = word_tokenize(tokens) # default tokenizer\n",
    "    tokens = [w for w in tokens if w not in stopwords.words('english')] # filter English stopwords\n",
    "    #tokens = [w for w in tokens if len(w) > 2]\n",
    "    #tokens = [porter.stem(tok) for tok in tokens] # apply stemmer\n",
    "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
    "    tokens = [w for w in tokens if w.isalpha()] # filter tokens that contain non-alphabetic character(s)\n",
    "    return tokens\n",
    "\n",
    "def tokenize(path):\n",
    "    # open PDF\n",
    "    pdf = PyPDF2.PdfFileReader(open(str(path),\"rb\"))\n",
    "    stopword_list = list(stopwords.words(\"english\"))\n",
    "\n",
    "    # read PDF file in a list\n",
    "    pdf_content = []\n",
    "    for page in pdf.pages:\n",
    "        pdf_content.append(page.extractText())\n",
    "    \n",
    "    # create a list of token\n",
    "    tokens = [None] * len(pdf_content)\n",
    "    for i in range(len(pdf_content)):\n",
    "        tokens[i] = clean_token(pdf_content[i])\n",
    "    tokens = [t for tok in tokens for t in tok] \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst = tokenize('../data/documents/resume test.pdf')\n",
    "#set(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names():\n",
    "    files = []\n",
    "    #'../data/solarhrm*.pdf'\n",
    "    for file in glob.glob(\"../data/documents/*.pdf\"):\n",
    "        files.append(file)\n",
    "    return files\n",
    "\n",
    "def make_index(tokens, document_name, index, length):\n",
    "    for term in set(tokens):\n",
    "        index[term].append([document_name,tokens.count(term)])\n",
    "        length[document_name] = len(set(tokens))\n",
    "\n",
    "# saving index into json        \n",
    "def write(inverted_index,length_index):\n",
    "    inv_index_file = open(\"../data/indexes/inverted_index.json\",\"w\")\n",
    "    json.dump(inverted_index,inv_index_file)\n",
    "\n",
    "    length_index_file = open(\"../data/indexes/length_index.json\",\"w\")\n",
    "    json.dump(length_index,length_index_file)\n",
    "    \n",
    "def generator():\n",
    "    resume_files = get_file_names()\n",
    "    inverted_index = defaultdict(list)\n",
    "    length_index = defaultdict(list)\n",
    "    for file in resume_files:\n",
    "        make_index(tokenize(file), file, inverted_index, length_index)\n",
    "    write(inverted_index,length_index)\n",
    "    print (\"Indexes generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create retrieval-The BM25 Weighting Scheme**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**formula**\n",
    "\n",
    "For query Q and document d, we have BM25 d of Q:\n",
    "$ BM25_{score}(Q,d)=\\sum_{t\\in Q}w(t,d) $\n",
    "\n",
    "$ w(t,d)=\\frac{(k_2+1)qf_i}{k_2+qf_i}\\times \\frac{(k_1+1)\\times f_i}{f_i+K}\\times log_2\\frac{(r_i+0.5)/(R-r_i+0.5)}{(n_i-r_i+0.5)/(N-n_i-R+r_i+0.5)} â€‹$\n",
    "\n",
    "\n",
    "* $r_i$ is the # of relevant documents containing term i \n",
    "* $n_i$  is the # of docs containing term i\n",
    "* $N$ is the total # of docs in the collection\n",
    "* $R$ is the number of relevant documents for this query  (set to 0 if no relevancy info is known)\n",
    "* $f_i$  is the frequency of term i in the doc under consideration\n",
    "* $qf_i$ is the frequency of term i in the query\n",
    "* $k_1$ determines how the tf component of the term weight changes as $f_i$\n",
    "  increases. (if 0, then tf component is ignored.) \n",
    "* $k_2$ has a similar role for the query term weights. Typical values make the equation less sensitive to k2 than k1 because query term frequencies are much lower and less variable than doc term frequencies.\n",
    "* $K$ It equals to ($k_1(1-b+b\\times l_d/avg\\_l)$). Its role is basically to normalize the tf component by document length.\n",
    "* $b$ regulates the impact of length normalization. (0 means none; 1 is full normalization.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "'''\n",
    "IR Book: 11.4.3\n",
    "Fomula: 11.33\n",
    "'''\n",
    "'''\n",
    "typical TREC value (Text Retrieval Conference (TREC).)\n",
    "f1 = 1.2\n",
    "k2 varies from 0 to 1000\n",
    "b = 0.75\n",
    "'''\n",
    "\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "k2 = 100\n",
    "R = 0 # (set it to 0 since no relevancy info is known)\n",
    "\n",
    "# MAIN METHOD\n",
    "\n",
    "def BM25(docLen, avDocLen, n, N, f, q, r):\n",
    "    p1 = ((k2 + 1) * q) / (k2 + q)\n",
    "    p2 = ((k1 + 1) * f) / (getK(docLen, avDocLen) + f)\n",
    "    p3 = log(((r + 0.5)/(R-r+0.5)) / ((n - r + 0.5)/(N - n - R + r + 0.5)))\n",
    "    return p1 * p2 * p3\n",
    "\n",
    "def getK(docLen, avDocLen):\n",
    "    return k1 * ((1 - b) + b * (float(docLen) / float(avDocLen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create Ranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average document length\n",
    "def get_avdl(length_index):\n",
    "    corpus_length = 0\n",
    "    for document in length_index:\n",
    "        corpus_length += length_index[document]\n",
    "    return float(corpus_length) / float(len(length_index))\n",
    "\n",
    "def search(query):\n",
    "    inv_index_file = open(\"../data/indexes/inverted_index.json\",\"r\")\n",
    "    inverted_index = json.load(inv_index_file)\n",
    "\n",
    "    length_index_file = open(\"../data/indexes/length_index.json\",\"r\")\n",
    "    length_index = json.load(length_index_file)\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    #query_tokens = query.split()\n",
    "    #for token in query_tokens:\n",
    "    for token in query:\n",
    "        if token in inverted_index.keys():\n",
    "            for entry in inverted_index[token]:\n",
    "                scores[entry[0]] = BM25(length_index[entry[0]],get_avdl(length_index),len(inverted_index[token]),len(length_index),entry[1],1,0)\n",
    "    return sorted(scores.items(),key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(keyword):\n",
    "    results = search(keyword)\n",
    "    \n",
    "    for result in results:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword = 'machine learning'\n",
    "matching('i like python and sql ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jd side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jd_1(dirt):\n",
    "    '''\n",
    "    Method 1: only keep noun in the job description\n",
    "    '''\n",
    "    lst = tokenize(dirt)\n",
    "    \n",
    "    client = language.LanguageServiceClient()\n",
    "    # part-of-speech tags from list(enums.PartOfSpeech.Tag)\n",
    "    pos_tag = ('UNKNOWN', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM',\n",
    "               'PRON', 'PRT', 'PUNCT', 'VERB', 'X', 'AFFIX')\n",
    "    tags = ['NOUN']\n",
    "    \n",
    "    output = []\n",
    "    for _ in lst:\n",
    "        #output =[]\n",
    "        #doc = ' '.join(_)\n",
    "        document = language.types.Document(content = _, type=enums.Document.Type.PLAIN_TEXT)\n",
    "        tokens = client.analyze_syntax(document).tokens\n",
    "        for token in tokens:\n",
    "            if pos_tag[token.part_of_speech.tag] in tags:\n",
    "                output.append(token.text.content)\n",
    "                \n",
    "    c = Counter(output)\n",
    "    query = [key for key, val in c.most_common(20)]\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jd_2(dirt):\n",
    "    '''\n",
    "    method 2: terms sorted by tf idf weights\n",
    "    '''\n",
    "    doc = tokenize(dirt)\n",
    "    \n",
    "    cv=CountVectorizer(stop_words=stopwords.words('english'))\n",
    "    word_count_vector=cv.fit_transform(doc)\n",
    "    \n",
    "    #calculate the weights for each term in each document\n",
    "    tfidf_transformer=TfidfTransformer()\n",
    "    tf_idf_vector = tfidf_transformer.fit_transform(word_count_vector)\n",
    "    #the top 20 terms by average tf-idf weight\n",
    "    weights = np.asarray(tf_idf_vector.mean(axis=0)).ravel().tolist()\n",
    "    weights_df = pd.DataFrame({'term': cv.get_feature_names(), 'weight': weights}).sort_values(by='weight', ascending=False).head(20)\n",
    "    return weights_df.term.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../data/documents\\\\resume test.pdf', 2.323120297319265),\n",
       " ('../data/documents\\\\106099246Kesapragada Simhadri Resume.pdf',\n",
       "  0.4812935993450918),\n",
       " ('../data/documents\\\\105820815Resume of Naren.pdf', -0.8774204188526596),\n",
       " ('../data/documents\\\\125699806Balindra Singh..GP.pdf', -1.0825686425824363),\n",
       " ('../data/documents\\\\113475465dhaya resume.pdf', -1.1137650089258129),\n",
       " ('../data/documents\\\\112163561ADAM_NAWAB.....pdf', -1.230191639298223),\n",
       " ('../data/documents\\\\113183139imran_B.tech_IT.pdf', -1.6849929539063035)]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirt = '../data/job_description/qualification.pdf'\n",
    "search(clean_jd_1(dirt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../data/documents\\\\resume test.pdf', 1.143448801586362),\n",
       " ('../data/documents\\\\105820815Resume of Naren.pdf', -0.8774204188526596),\n",
       " ('../data/documents\\\\125699806Balindra Singh..GP.pdf', -1.0825686425824363),\n",
       " ('../data/documents\\\\113475465dhaya resume.pdf', -1.1137650089258129),\n",
       " ('../data/documents\\\\112163561ADAM_NAWAB.....pdf', -1.230191639298223),\n",
       " ('../data/documents\\\\113183139imran_B.tech_IT.pdf', -1.6849929539063035)]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirt = '../data/job_description/qualification.pdf'\n",
    "search(clean_jd_2(dirt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
