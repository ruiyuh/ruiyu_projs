{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC575 Project-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruiyu Hu 2019-03-18 \n",
      "\n",
      "CPython 3.6.7\n",
      "IPython 7.0.1\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Ruiyu Hu\" -d -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pdf file\n",
    "import PyPDF2 \n",
    "\n",
    "# tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from textblob import TextBlob # pos_tag\n",
    "\n",
    "# create inverse index\n",
    "\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import operator\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file tokenize.py\n",
    "def get_file_names():\n",
    "    files = []\n",
    "    for file in glob.glob(\"../data/documents/*.pdf\"):\n",
    "        files.append(file)\n",
    "    return files\n",
    "#get_file_names()\n",
    "###########################################################################\n",
    "def convert(file):\n",
    "    pdf_content = []\n",
    "\n",
    "    pdf = PyPDF2.PdfFileReader(open(str(file),\"rb\"))\n",
    "    # pdf may be more than one page\n",
    "    num_pages = pdf.numPages\n",
    "    count = 0\n",
    "    text = ''\n",
    "    while count < num_pages:\n",
    "        pageObj = pdf.getPage(count)\n",
    "        count +=1\n",
    "        text += pageObj.extractText().replace('\\n','')\n",
    "    if text != '':\n",
    "        text = text\n",
    "            \n",
    "    pdf_content.append(text)   \n",
    "    return pdf_content\n",
    "\n",
    "'''def convert(file):\n",
    "    text = []\n",
    "    with open(str(file), 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            text.append(line.decode(\"utf-8\", \"ignore\").strip())\n",
    "    return text'''\n",
    "\n",
    "###########################################################################     \n",
    "def tokenize(lst):            \n",
    "    # create a list of token\n",
    "    \n",
    "    tokens = [None] * len(lst)\n",
    "    for i in range(len(lst)):\n",
    "        tokens[i] = clean_token(lst[i])\n",
    "    tokens = [t for tok in tokens for t in tok] \n",
    "    return tokens\n",
    "\n",
    "########################################################################### \n",
    "def clean_token(text):\n",
    "    #porter = nltk.PorterStemmer()\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    #snowball = nltk.SnowballStemmer('english')\n",
    "        \n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stopset.update(('year'))\n",
    "    \n",
    "    noun_lst = []\n",
    "\n",
    "    for word,tag in (TextBlob(text).tags):\n",
    "        if tag in (\"NN\", \"NNS\", \"NNP\", \"NNPS\",\"JJ\"):\n",
    "            word = word.lower()\n",
    "            word = lemmatizer.lemmatize(word, pos = 'n')\n",
    "            #word = porter.stem(word)\n",
    "            #word = snowball.stem(word)\n",
    "            if word not in stopset and word.isalpha() and len(word)>2:\n",
    "                noun_lst.append(word)\n",
    "    return noun_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for _ in get_file_names()[:2]:\n",
    "    #print(convert(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for _ in get_file_names()[:2]:\n",
    "    #print(tokenize(convert(_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# set up env locally\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r'C:\\Users\\RayHu\\ruiyu-gcp-4ac10836d3b1.json' \n",
    "\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "def pos_clean(dir):\n",
    "    lst = tokenize(dirt)\n",
    "    \n",
    "    client = language.LanguageServiceClient()\n",
    "    # part-of-speech tags from list(enums.PartOfSpeech.Tag)\n",
    "    pos_tag = ('UNKNOWN', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM',\n",
    "               'PRON', 'PRT', 'PUNCT', 'VERB', 'X', 'AFFIX')\n",
    "    tags = ['NOUN']\n",
    "    \n",
    "    output = []\n",
    "    for _ in lst:\n",
    "        #output =[]\n",
    "        #doc = ' '.join(_)\n",
    "        document = language.types.Document(content = _, type=enums.Document.Type.PLAIN_TEXT)\n",
    "        tokens = client.analyze_syntax(document).tokens\n",
    "        for token in tokens:\n",
    "            if pos_tag[token.part_of_speech.tag] in tags:\n",
    "                output.append(token.text.content)\n",
    "                \n",
    "    c = Counter(output)\n",
    "    query = [key for key, val in c.most_common(20)]\n",
    "    \n",
    "    return query'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_idx(tokens, doc_name, idx, length):\n",
    "    for term in set(tokens):\n",
    "        idx[term].append([doc_name,tokens.count(term)])\n",
    "        length[doc_name] = len(set(tokens))\n",
    "\n",
    "def write(inverted_idx,len_idx):\n",
    "    inv_idx_file = open(\"../data/indexes/inverted_idx.json\",\"w\")\n",
    "    json.dump(inverted_idx,inv_idx_file)\n",
    "\n",
    "    len_idx_file = open(\"../data/indexes/len_idx.json\",\"w\")\n",
    "    json.dump(len_idx,len_idx_file)\n",
    "    \n",
    "def generate_idx():\n",
    "    resume_files = get_file_names()\n",
    "    inverted_index = defaultdict(list)\n",
    "    length_index = defaultdict(list)\n",
    "    \n",
    "    for file in resume_files:\n",
    "        make_idx(tokenize(convert(file)), file, inverted_index, length_index)\n",
    "        \n",
    "    write(inverted_index,length_index)\n",
    "    print (\"Indexes generated\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes generated\n"
     ]
    }
   ],
   "source": [
    "generate_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inv_idx_file = open(\"../data/indexes/inverted_idx.json\",\"r\")\n",
    "#inverted_idx = json.load(inv_idx_file)\n",
    "#inverted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len_idx_file = open(\"../data/indexes/len_idx.json\",\"r\")\n",
    "#len_idx = json.load(len_idx_file)\n",
    "#len_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''inv_idx_file = open(\"../data/indexes/inverted_idx.json\",\"r\")\n",
    "inv_indx = json.load(inv_idx_file)\n",
    "# create dictionary\n",
    "doc_freq ={}\n",
    "for key in sorted(inv_indx.keys()):\n",
    "    doc_freq[key] = sum(Counter(set(doc_id for doc_id, term in inv_indx[key])).values())\n",
    "\n",
    "dictionary = pd.DataFrame.from_dict(doc_freq,orient='index', columns=['DocFreq'])\n",
    "dictionary.head()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BM25**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For query Q and document d, we have BM25 d of Q:\n",
    "\n",
    "$  BM25_{score}(Q,d)=\\frac{(k_2+1)qf_i}{k_2+qf_i}\\times \\frac{(k_1+1)\\times f_i}{f_i+k_1(1-b+b\\times l_d/avg\\_l)}\\times log_2\\frac{(r_i+0.5)/(R-r_i+0.5)}{(n_i-r_i+0.5)/(N-n_i-R+r_i+0.5)} â€‹$\n",
    "\n",
    "\n",
    "* $r_i$ is the # of relevant documents containing term i \n",
    "* $n_i$  is the # of docs containing term i\n",
    "* N is the total # of docs in the collection\n",
    "* R is the number of relevant documents for this query  (set to 0 if no relevancy info is known)\n",
    "* $f_i$  is the frequency of term i in the doc under consideration\n",
    "* $qf_i$ is the frequency of term i in the query\n",
    "* $k_1$ determines how the tf component of the term weight changes as $f_i$\n",
    "  increases. (if 0, then tf component is ignored.) \n",
    "* $k_2$ has a similar role for the query term weights. Typical values make the equation less sensitive to k2 than k1 because query term frequencies are much lower and less variable than doc term frequencies.\n",
    "* K ($k_1(1-b+b\\times l_d/avg\\_l)$) is more complicated. Its role is basically to normalize the tf component by document length.\n",
    "* b regulates the impact of length normalization. (0 means none; 1 is full normalization.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "'''\n",
    "IR Book: 11.4.3\n",
    "Fomula: 11.33\n",
    "'''\n",
    "'''\n",
    "typical TREC value (Text Retrieval Conference (TREC).)\n",
    "f1 = 1.2\n",
    "k2 varies from 0 to 1000\n",
    "b = 0.75\n",
    "'''\n",
    "\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "k2 = 100\n",
    "R = 0 # (set it to 0 since no relevancy info is known)\n",
    "\n",
    "\n",
    "\n",
    "def BM25(doc_len, avg_doc_len, n_doc_w_term, n_total_doc, freq_term_doc, freq_term_query, rel_doc_w_term):\n",
    "    \n",
    "    n = n_doc_w_term\n",
    "    N = n_total_doc\n",
    "    f = freq_term_doc\n",
    "    q = freq_term_query\n",
    "    r = rel_doc_w_term\n",
    "    \n",
    "    p1 = ((k2 + 1) * q) / (k2 + q) #Relevance between term and query\n",
    "    p2 = ((k1 + 1) * f) / (getK(doc_len, avg_doc_len) + f) #Relevance between term and document\n",
    "    p3 = log((((r + 0.5)/(R-r+0.5)) / ((n - r + 0.5)/(N - n - R + r + 0.5)))+1) # Term Weight\n",
    "    return p1 * p2 * p3\n",
    "\n",
    "def getK(doc_len, avg_doc_len):\n",
    "    return k1 * ((1 - b) + b * (float(doc_len) / float(avg_doc_len)))\n",
    "    \n",
    "#def sigmoid(x):\n",
    "    #return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average document length\n",
    "def get_avg_doc_len(len_idx):\n",
    "    _length = 0\n",
    "    for doc in len_idx:\n",
    "        _length += len_idx[doc]\n",
    "    return float(_length) / float(len(len_idx))\n",
    "\n",
    "def search(query):\n",
    "    inv_idx_file = open(\"../data/indexes/inverted_idx.json\",\"r\")\n",
    "    inverted_idx = json.load(inv_idx_file)\n",
    "\n",
    "    len_idx_file = open(\"../data/indexes/len_idx.json\",\"r\")\n",
    "    len_idx = json.load(len_idx_file)\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    query_tokens = query.split()\n",
    "    for token in query_tokens:\n",
    "    #for token in query:\n",
    "        #token = token.lower()\n",
    "        for tok in clean_token(token):\n",
    "            if tok in inverted_idx.keys():\n",
    "                for entry in inverted_idx[tok]:\n",
    "                    bm25_val = BM25(len_idx[entry[0]],get_avg_doc_len(len_idx),len(inverted_idx[tok]),len(len_idx),entry[1],1,0)\n",
    "                    #scores[entry[0]] = round(10* sigmoid(bm25_val)-5,4)\n",
    "                    scores[entry[0]] = round(bm25_val,4)\n",
    "    result = sorted(scores.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('../data/documents\\\\ml002.pdf', 0.8568),\n",
       " ('../data/documents\\\\ml004.pdf', 0.8517),\n",
       " ('../data/documents\\\\ml003.pdf', 0.8479),\n",
       " ('../data/documents\\\\ml005.pdf', 0.8459),\n",
       " ('../data/documents\\\\ml001.pdf', 0.7893)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#search('machine learning')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cosine SImilarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Loading word2vec model cost 86.375 seconds...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from gensim.models import KeyedVectors\n",
    "t1 = time.time()\n",
    "#download link: https://github.com/mmihaltz/word2vec-GoogleNews-vectors\n",
    "path = 'C:/Users/RayHu/Downloads/google_w2v/GoogleNews-vectors-negative300.bin'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "print('-------------------------------------------')\n",
    "print(\"Loading word2vec model cost %.3f seconds...\\n\" % (time.time() - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file vectorize.py\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def vectorize(words):\n",
    "    '''\n",
    "    transform the doc and query into vectors\n",
    "    '''\n",
    "    word_vec = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec = w2v_model[word]\n",
    "            word_vec.append(vec)\n",
    "        except KeyError:\n",
    "            # ignore the word if it is not in the w2v vocablary\n",
    "            pass\n",
    "    vector = np.mean(word_vec,axis = 0)\n",
    "    return vector\n",
    "\n",
    "def cos_sim(vector1, vector2):\n",
    "    '''\n",
    "    the fomula to calculte cosine cimilarity \n",
    "    '''\n",
    "    sim = np.dot(vector1, vector2) / (LA.norm(vector1) * LA.norm(vector2))\n",
    "    \n",
    "    if np.isnan(np.sum(sim)):\n",
    "        return 0\n",
    "    \n",
    "    return sim\n",
    "\n",
    "def calc_sim(query):\n",
    "    '''\n",
    "    calculate similarity scores between documents and the query\n",
    "    '''\n",
    "    query = clean_token(query)\n",
    "    file_list = get_file_names()\n",
    "    documents = {}\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        documents[file_list[i]] = tokenize(convert(file_list[i]))\n",
    "        \n",
    "    query_vec = vectorize(query)\n",
    "    results = {}\n",
    "    \n",
    "    for name, doc in documents.items():\n",
    "        doc_vec = vectorize(doc)\n",
    "        sim_score = cos_sim(query_vec, doc_vec)\n",
    "        #threshold = 0.5\n",
    "        if sim_score > 0:\n",
    "            results[name] = round(sim_score,4)\n",
    "            sort_result = sorted(results.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sort_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc_sim('machine learning')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jaccard Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(query, documents):\n",
    "    '''\n",
    "    the fomula to calculate Jaccard\n",
    "    '''\n",
    "    s1 = set(query)\n",
    "    s2 = set(documents)\n",
    "    lst1 = s1.intersection(s2)\n",
    "    lst2 = s1.union(s2)\n",
    "    jaccard = 1.0 * len(lst1)/len(lst2)\n",
    "    \n",
    "    return jaccard\n",
    "\n",
    "def calc_jac(query):\n",
    "    query = set(clean_token(query))\n",
    "    \n",
    "    file_list = get_file_names()\n",
    "    documents = {}\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        documents[file_list[i]] = set(tokenize(convert(file_list[i])))\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, doc in documents.items():\n",
    "        jac = jaccard(query, doc)\n",
    "        results[name] = round(jac,4)\n",
    "        sort_result = sorted(results.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sort_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc_jac('machine learning')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(query):\n",
    "    #print(query)\n",
    "    print('-------------------------------------------------------')\n",
    "    t1 = time.time()\n",
    "    results_bm25 = search(query)[:3]\n",
    "    print('bm25 result')\n",
    "    for result in results_bm25:\n",
    "        print(result)\n",
    "    print('The computing cost %.3f seconds\\n'% (time.time() - t1))\n",
    "    print('-------------------------------------------------------')\n",
    "    t2 = time.time()\n",
    "    results_cos = calc_sim(query)[:3]\n",
    "    print('cosine result')\n",
    "    for result in results_cos:\n",
    "        print(result)\n",
    "    print('The computing cost %.3f seconds\\n'% (time.time() - t2))   \n",
    "    print('-------------------------------------------------------')\n",
    "    t3 = time.time()\n",
    "    results_jac = calc_jac(query)[:3]\n",
    "    print('jacarrd result')\n",
    "    for result in results_jac:\n",
    "        print(result)\n",
    "    print('The computing cost %.3f seconds\\n'% (time.time() - t3))\n",
    "    print('-------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test for the long query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_ds_jd = 'Apply advanced statistical techniques to model user behavior, identify causal impact and attribution, build and benchmark metrics.Write complex data flows using SQL, Spark, Scalding, R and Python scripts.Use data visualization tools (e.g, Tableau, Zeppelin) to share ongoing insights.'\n",
    "adobe_ml_jd = 'Hands on experience with Java, Python, and/or C++.Work on machine learning models & algorithms, web services, distributed systems, data mining, big data, Hadoop, deep learning, recommendations, and more by developing a Machine Platform at Adobe that would power Adobe Clouds.Apply data mining and machine learning to improve content understanding, computer vision, deep learning, language understanding and content ranking & recommendations.Maintain and optimize machine learning platform, identify new ideas to evolve it, develop new features and benchmark possible solutions.Build machine learning capabilities using technologies such as REST web services, micro-services, Caffe, Tensorflow, Spark, Elastic, AWS, Kafka, Deep Learning, Matlab, R, and more.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply advanced statistical techniques to model user behavior, identify causal impact and attribution, build and benchmark metrics.Write complex data flows using SQL, Spark, Scalding, R and Python scripts.Use data visualization tools (e.g, Tableau, Zeppelin) to share ongoing insights.\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\ds002.pdf', 1.361)\n",
      "('../data/documents\\\\de005.pdf', 1.3028)\n",
      "('../data/documents\\\\ds001.pdf', 1.2891)\n",
      "The computing cost 0.180 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\ds002.pdf', 0.8088)\n",
      "('../data/documents\\\\ds001.pdf', 0.8001)\n",
      "('../data/documents\\\\ds005.pdf', 0.797)\n",
      "The computing cost 3.793 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\ds003.pdf', 0.0663)\n",
      "('../data/documents\\\\ds002.pdf', 0.0618)\n",
      "('../data/documents\\\\de001.pdf', 0.0588)\n",
      "The computing cost 4.904 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "Hands on experience with Java, Python, and/or C++.Work on machine learning models & algorithms, web services, distributed systems, data mining, big data, Hadoop, deep learning, recommendations, and more by developing a Machine Platform at Adobe that would power Adobe Clouds.Apply data mining and machine learning to improve content understanding, computer vision, deep learning, language understanding and content ranking & recommendations.Maintain and optimize machine learning platform, identify new ideas to evolve it, develop new features and benchmark possible solutions.Build machine learning capabilities using technologies such as REST web services, micro-services, Caffe, Tensorflow, Spark, Elastic, AWS, Kafka, Deep Learning, Matlab, R, and more.\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\da003.pdf', 1.8837)\n",
      "('../data/documents\\\\ds003.pdf', 1.8115)\n",
      "('../data/documents\\\\de002.pdf', 1.7156)\n",
      "The computing cost 0.283 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\ml003.pdf', 0.8623)\n",
      "('../data/documents\\\\ml001.pdf', 0.8547)\n",
      "('../data/documents\\\\ml002.pdf', 0.849)\n",
      "The computing cost 4.183 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\ml005.pdf', 0.1138)\n",
      "('../data/documents\\\\ds004.pdf', 0.106)\n",
      "('../data/documents\\\\ml001.pdf', 0.0923)\n",
      "The computing cost 3.851 seconds\n",
      "\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "queries=[twitter_ds_jd, adobe_ml_jd]    \n",
    "for query in queries:\n",
    "    print(query)\n",
    "    matching(query)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"queries=['machine learning','data science','data analysis','familiar wth python, sql and r']    \\nfor query in queries:\\n    print(query)\\n    matching(query)\\n    continue\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''queries=['machine learning','data science','data analysis','familiar wth python, sql and r']    \n",
    "for query in queries:\n",
    "    print(query)\n",
    "    matching(query)\n",
    "    continue'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
