{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruiyu Hu 2019-03-18 \n",
      "\n",
      "CPython 3.6.7\n",
      "IPython 7.0.1\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Ruiyu Hu\" -d -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pdf file\n",
    "import PyPDF2 \n",
    "\n",
    "# tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# create inverse index\n",
    "\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import operator\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file tokenize.py\n",
    "def get_file_names():\n",
    "    files = []\n",
    "    for file in glob.glob(\"../data/documents/*.pdf\"):\n",
    "        files.append(file)\n",
    "    return files\n",
    "#get_file_names()\n",
    "###########################################################################\n",
    "def convert(file):\n",
    "    pdf_content = []\n",
    "\n",
    "    pdf = PyPDF2.PdfFileReader(open(str(file),\"rb\"))\n",
    "    # pdf may be more than one page\n",
    "    num_pages = pdf.numPages\n",
    "    count = 0\n",
    "    text = ''\n",
    "    while count < num_pages:\n",
    "        pageObj = pdf.getPage(count)\n",
    "        count +=1\n",
    "        text += pageObj.extractText().replace('\\n','')\n",
    "    if text != '':\n",
    "        text = text\n",
    "            \n",
    "    pdf_content.append(text)   \n",
    "    return pdf_content\n",
    "\n",
    "'''def convert(file):\n",
    "    text = []\n",
    "    with open(str(file), 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            text.append(line.decode(\"utf-8\", \"ignore\").strip())\n",
    "    return text'''\n",
    "\n",
    "###########################################################################     \n",
    "def tokenize(lst):            \n",
    "    # create a list of token\n",
    "    \n",
    "    tokens = [None] * len(lst)\n",
    "    for i in range(len(lst)):\n",
    "        tokens[i] = clean_token(lst[i])\n",
    "    tokens = [t for tok in tokens for t in tok] \n",
    "    return tokens\n",
    "\n",
    "########################################################################### \n",
    "def clean_token(text):\n",
    "    #porter = nltk.PorterStemmer()\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    #snowball = nltk.SnowballStemmer('english')\n",
    "        \n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stopset.update(('less','year'))\n",
    "    \n",
    "    noun_lst = []\n",
    "\n",
    "    for word,tag in (TextBlob(text).tags):\n",
    "        if tag in (\"NN\", \"NNS\", \"NNP\", \"NNPS\",\"JJ\"):\n",
    "            word = word.lower()\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            #word = porter.stem(word)\n",
    "            #word = snowball.stem(word)\n",
    "            if word not in stopset and word.isalpha() and len(word)>2:\n",
    "                noun_lst.append(word)\n",
    "    return noun_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_idx(tokens, doc_name, idx, length):\n",
    "    for term in set(tokens):\n",
    "        idx[term].append([doc_name,tokens.count(term)])\n",
    "        length[doc_name] = len(set(tokens))\n",
    "\n",
    "def write(inverted_idx,len_idx):\n",
    "    inv_idx_file = open(\"../data/indexes/inverted_idx.json\",\"w\")\n",
    "    json.dump(inverted_idx,inv_idx_file)\n",
    "\n",
    "    len_idx_file = open(\"../data/indexes/len_idx.json\",\"w\")\n",
    "    json.dump(len_idx,len_idx_file)\n",
    "    \n",
    "def generate_idx():\n",
    "    resume_files = get_file_names()\n",
    "    inverted_index = defaultdict(list)\n",
    "    length_index = defaultdict(list)\n",
    "    \n",
    "    for file in resume_files:\n",
    "        make_idx(tokenize(convert(file)), file, inverted_index, length_index)\n",
    "        \n",
    "    write(inverted_index,length_index)\n",
    "    print (\"Indexes generated\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes generated\n"
     ]
    }
   ],
   "source": [
    "generate_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''inv_idx_file = open(\"../data/indexes/inverted_idx.json\",\"r\")\n",
    "inv_indx = json.load(inv_idx_file)\n",
    "# create dictionary\n",
    "doc_freq ={}\n",
    "for key in sorted(inv_indx.keys()):\n",
    "    doc_freq[key] = sum(Counter(set(doc_id for doc_id, term in inv_indx[key])).values())\n",
    "\n",
    "dictionary = pd.DataFrame.from_dict(doc_freq,orient='index', columns=['DocFreq'])\n",
    "dictionary.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create retrieval-The BM25 Weighting Scheme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "'''\n",
    "IR Book: 11.4.3\n",
    "Fomula: 11.33\n",
    "'''\n",
    "'''\n",
    "typical TREC value (Text Retrieval Conference (TREC).)\n",
    "f1 = 1.2\n",
    "k2 varies from 0 to 1000\n",
    "b = 0.75\n",
    "'''\n",
    "\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "k2 = 100\n",
    "R = 0 # (set it to 0 since no relevancy info is known)\n",
    "\n",
    "\n",
    "\n",
    "def BM25(doc_len, avg_doc_len, n_doc_w_term, n_total_doc, freq_term_doc, freq_term_query, rel_doc_w_term):\n",
    "    \n",
    "    n = n_doc_w_term\n",
    "    N = n_total_doc\n",
    "    f = freq_term_doc\n",
    "    q = freq_term_query\n",
    "    r = rel_doc_w_term\n",
    "    \n",
    "    p1 = ((k2 + 1) * q) / (k2 + q) #Relevance between term and query\n",
    "    p2 = ((k1 + 1) * f) / (getK(doc_len, avg_doc_len) + f) #Relevance between term and document\n",
    "    p3 = log((((r + 0.5)/(R-r+0.5)) / ((n - r + 0.5)/(N - n - R + r + 0.5)))+1) # Term Weight\n",
    "    return p1 * p2 * p3\n",
    "\n",
    "def getK(doc_len, avg_doc_len):\n",
    "    return k1 * ((1 - b) + b * (float(doc_len) / float(avg_doc_len)))\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average document length\n",
    "def get_avg_doc_len(len_idx):\n",
    "    _length = 0\n",
    "    for doc in len_idx:\n",
    "        _length += len_idx[doc]\n",
    "    return float(_length) / float(len(len_idx))\n",
    "\n",
    "def search(query):\n",
    "    inv_idx_file = open(\"../data/indexes/inverted_idx.json\",\"r\")\n",
    "    inverted_idx = json.load(inv_idx_file)\n",
    "\n",
    "    len_idx_file = open(\"../data/indexes/len_idx.json\",\"r\")\n",
    "    len_idx = json.load(len_idx_file)\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    query_tokens = query.split()\n",
    "    for token in query_tokens:\n",
    "    #for token in query:\n",
    "        #token = token.lower()\n",
    "        for tok in clean_token(token):\n",
    "            if tok in inverted_idx.keys():\n",
    "                for entry in inverted_idx[tok]:\n",
    "                    bm25_val = BM25(len_idx[entry[0]],get_avg_doc_len(len_idx),len(inverted_idx[tok]),len(len_idx),entry[1],1,0)\n",
    "                #scores[entry[0]] = round(10* sigmoid(bm25_val)-5,4)\n",
    "                    scores[entry[0]] = round(bm25_val,4)\n",
    "    result = sorted(scores.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**w2v approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Loading word2vec model cost 86.375 seconds...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from gensim.models import KeyedVectors\n",
    "t1 = time.time()\n",
    "#download link: https://github.com/mmihaltz/word2vec-GoogleNews-vectors\n",
    "path = 'C:/Users/RayHu/Downloads/google_w2v/GoogleNews-vectors-negative300.bin'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "print('-------------------------------------------')\n",
    "print(\"Loading word2vec model cost %.3f seconds...\\n\" % (time.time() - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file vectorize.py\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def vectorize(words):\n",
    "    '''\n",
    "    transform the doc and query into vectors\n",
    "    '''\n",
    "    word_vec = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec = w2v_model[word]\n",
    "            word_vec.append(vec)\n",
    "        except KeyError:\n",
    "            # ignore the word if it is not in the w2v vocablary\n",
    "            pass\n",
    "    vector = np.mean(word_vec,axis = 0)\n",
    "    return vector\n",
    "\n",
    "def cos_sim(vector1, vector2):\n",
    "    '''\n",
    "    the fomula to calculte cosine cimilarity \n",
    "    '''\n",
    "    sim = np.dot(vector1, vector2) / (LA.norm(vector1) * LA.norm(vector2))\n",
    "    \n",
    "    if np.isnan(np.sum(sim)):\n",
    "        return 0\n",
    "    \n",
    "    return sim\n",
    "\n",
    "def calc_sim(query):\n",
    "    '''\n",
    "    calculate similarity scores between documents and the query\n",
    "    '''\n",
    "    query = clean_token(query)\n",
    "    file_list = get_file_names()\n",
    "    documents = {}\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        documents[file_list[i]] = tokenize(convert(file_list[i]))\n",
    "        \n",
    "    query_vec = vectorize(query)\n",
    "    results = {}\n",
    "    \n",
    "    for name, doc in documents.items():\n",
    "        doc_vec = vectorize(doc)\n",
    "        sim_score = cos_sim(query_vec, doc_vec)\n",
    "        #threshold = 0.5\n",
    "        if sim_score > 0:\n",
    "            results[name] = sim_score\n",
    "            sort_result = sorted(results.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sort_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(query, documents):\n",
    "    s1 = set(query)\n",
    "    s2 = set(documents)\n",
    "    lst1 = s1.intersection(s2)\n",
    "    lst2 = s1.union(s2)\n",
    "    jaccard = 1.0 * len(lst1)/len(lst2)\n",
    "    \n",
    "    return jaccard\n",
    "\n",
    "def calc_jac(query):\n",
    "    query = set(clean_token(query))\n",
    "    \n",
    "    file_list = get_file_names()\n",
    "    documents = {}\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        documents[file_list[i]] = set(tokenize(convert(file_list[i])))\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, doc in documents.items():\n",
    "        jac = jaccard(query, doc)\n",
    "        results[name] = round(jac,4)\n",
    "        sort_result = sorted(results.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sort_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create Ranker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(query):\n",
    "    #print(query)\n",
    "    print('-------------------------------------------------------')\n",
    "    t1 = time.time()\n",
    "    results_bm25 = search(query)[:3]\n",
    "    print('bm25 result')\n",
    "    for result in results_bm25:\n",
    "        print(result)\n",
    "    print('The computing cost %.3f seconds\\n'% (time.time() - t1))\n",
    "    print('-------------------------------------------------------')\n",
    "    t2 = time.time()\n",
    "    results_cos = calc_sim(query)[:3]\n",
    "    print('cosine result')\n",
    "    for result in results_cos:\n",
    "        print(result)\n",
    "    print('The computing cost %.3f seconds\\n'% (time.time() - t2))   \n",
    "    print('-------------------------------------------------------')\n",
    "    t3 = time.time()\n",
    "    results_jac = calc_jac(query)[:3]\n",
    "    print('jacarrd result')\n",
    "    for result in results_jac:\n",
    "        print(result)\n",
    "    print('The computing cost %.3f seconds\\n'% (time.time() - t3))\n",
    "    print('-------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_ds_jd = 'Apply advanced statistical techniques to model user behavior, identify causal impact and attribution, build and benchmark metrics.Write complex data flows using SQL, Spark, Scalding, R and Python scripts.Use data visualization tools (e.g, Tableau, Zeppelin) to share ongoing insights.'\n",
    "adobe_ml_jd = 'Hands on experience with Java, Python, and/or C++.Work on machine learning models & algorithms, web services, distributed systems, data mining, big data, Hadoop, deep learning, recommendations, and more by developing a Machine Platform at Adobe that would power Adobe Clouds.Apply data mining and machine learning to improve content understanding, computer vision, deep learning, language understanding and content ranking & recommendations.Maintain and optimize machine learning platform, identify new ideas to evolve it, develop new features and benchmark possible solutions.Build machine learning capabilities using technologies such as REST web services, micro-services, Caffe, Tensorflow, Spark, Elastic, AWS, Kafka, Deep Learning, Matlab, R, and more.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply advanced statistical techniques to model user behavior, identify causal impact and attribution, build and benchmark metrics.Write complex data flows using SQL, Spark, Scalding, R and Python scripts.Use data visualization tools (e.g, Tableau, Zeppelin) to share ongoing insights.\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\ds002.pdf', 1.361)\n",
      "('../data/documents\\\\de005.pdf', 1.3028)\n",
      "('../data/documents\\\\ds001.pdf', 1.2891)\n",
      "The computing cost 0.124 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\ds002.pdf', 0.80875266)\n",
      "('../data/documents\\\\ds001.pdf', 0.80007637)\n",
      "('../data/documents\\\\ds005.pdf', 0.797029)\n",
      "The computing cost 3.852 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\ds003.pdf', 0.06626506024096386)\n",
      "('../data/documents\\\\ds002.pdf', 0.06179775280898876)\n",
      "('../data/documents\\\\de001.pdf', 0.058823529411764705)\n",
      "The computing cost 3.768 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "Hands on experience with Java, Python, and/or C++.Work on machine learning models & algorithms, web services, distributed systems, data mining, big data, Hadoop, deep learning, recommendations, and more by developing a Machine Platform at Adobe that would power Adobe Clouds.Apply data mining and machine learning to improve content understanding, computer vision, deep learning, language understanding and content ranking & recommendations.Maintain and optimize machine learning platform, identify new ideas to evolve it, develop new features and benchmark possible solutions.Build machine learning capabilities using technologies such as REST web services, micro-services, Caffe, Tensorflow, Spark, Elastic, AWS, Kafka, Deep Learning, Matlab, R, and more.\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\da003.pdf', 1.8837)\n",
      "('../data/documents\\\\ds003.pdf', 1.8115)\n",
      "('../data/documents\\\\de002.pdf', 1.7156)\n",
      "The computing cost 0.164 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\ml003.pdf', 0.86234355)\n",
      "('../data/documents\\\\ml001.pdf', 0.85472435)\n",
      "('../data/documents\\\\ml002.pdf', 0.84897816)\n",
      "The computing cost 4.459 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\ml005.pdf', 0.11382113821138211)\n",
      "('../data/documents\\\\ds004.pdf', 0.10596026490066225)\n",
      "('../data/documents\\\\ml001.pdf', 0.09230769230769231)\n",
      "The computing cost 4.129 seconds\n",
      "\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "queries=[twitter_ds_jd, adobe_ml_jd]    \n",
    "for query in queries:\n",
    "    print(query)\n",
    "    matching(query)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine learning\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\ml002.pdf', 0.8568)\n",
      "('../data/documents\\\\ml004.pdf', 0.8517)\n",
      "('../data/documents\\\\ml003.pdf', 0.8479)\n",
      "The computing cost 0.014 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\ml005.pdf', 0.6052575)\n",
      "('../data/documents\\\\ml003.pdf', 0.577961)\n",
      "('../data/documents\\\\ml001.pdf', 0.5635283)\n",
      "The computing cost 3.776 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\ml005.pdf', 0.020833333333333332)\n",
      "('../data/documents\\\\ml001.pdf', 0.019801980198019802)\n",
      "('../data/documents\\\\ml003.pdf', 0.017094017094017096)\n",
      "The computing cost 3.852 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "data visualization\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\de005.pdf', 1.7485)\n",
      "('../data/documents\\\\ds002.pdf', 1.5699)\n",
      "('../data/documents\\\\da005.pdf', 1.0854)\n",
      "The computing cost 0.009 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\ds002.pdf', 0.7233002)\n",
      "('../data/documents\\\\da001.pdf', 0.7061843)\n",
      "('../data/documents\\\\de004.pdf', 0.70204085)\n",
      "The computing cost 4.014 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\da005.pdf', 0.015748031496062992)\n",
      "('../data/documents\\\\da001.pdf', 0.014705882352941176)\n",
      "('../data/documents\\\\ml002.pdf', 0.013605442176870748)\n",
      "The computing cost 4.183 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "insight analysis\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\de002.pdf', 1.1467)\n",
      "('../data/documents\\\\ds004.pdf', 1.0886)\n",
      "('../data/documents\\\\ds002.pdf', 0.6117)\n",
      "The computing cost 0.011 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\ds002.pdf', 0.52086306)\n",
      "('../data/documents\\\\ds001.pdf', 0.49408904)\n",
      "('../data/documents\\\\de005.pdf', 0.49075913)\n",
      "The computing cost 3.948 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\de001.pdf', 0.01652892561983471)\n",
      "('../data/documents\\\\ds002.pdf', 0.012048192771084338)\n",
      "('../data/documents\\\\de003.pdf', 0.011363636363636364)\n",
      "The computing cost 3.884 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "big data platform\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\ds004.pdf', 2.5591)\n",
      "('../data/documents\\\\ds002.pdf', 1.6928)\n",
      "('../data/documents\\\\de005.pdf', 1.5912)\n",
      "The computing cost 0.012 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\da001.pdf', 0.7117415)\n",
      "('../data/documents\\\\de004.pdf', 0.7079551)\n",
      "('../data/documents\\\\ds002.pdf', 0.6885035)\n",
      "The computing cost 4.038 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\ds004.pdf', 0.023809523809523808)\n",
      "('../data/documents\\\\ds002.pdf', 0.018072289156626505)\n",
      "('../data/documents\\\\de003.pdf', 0.011235955056179775)\n",
      "The computing cost 4.100 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "familiar wth python, sql and r\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\de001.pdf', 0.4516)\n",
      "('../data/documents\\\\da001.pdf', 0.4457)\n",
      "('../data/documents\\\\da005.pdf', 0.4172)\n",
      "The computing cost 0.019 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\ml004.pdf', 0.5313201)\n",
      "('../data/documents\\\\de002.pdf', 0.52595735)\n",
      "('../data/documents\\\\de001.pdf', 0.5221822)\n",
      "The computing cost 3.870 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\de003.pdf', 0.02247191011235955)\n",
      "('../data/documents\\\\ml001.pdf', 0.019417475728155338)\n",
      "('../data/documents\\\\ml003.pdf', 0.01680672268907563)\n",
      "The computing cost 4.011 seconds\n",
      "\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "queries=['machine learning','data visualization','insight analysis',\n",
    "        'big data platform','familiar wth python, sql and r']    \n",
    "for query in queries:\n",
    "    print(query)\n",
    "    matching(query)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter search query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "::  data analysis and data visualization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data analysis and data visualization\n",
      "-------------------------------------------------------\n",
      "bm25 result\n",
      "('../data/documents\\\\de005.pdf', 1.748544547393187)\n",
      "('../data/documents\\\\ds002.pdf', 1.5699147549098849)\n",
      "('../data/documents\\\\da005.pdf', 1.0854099291059434)\n",
      "('../data/documents\\\\da001.pdf', 1.0571608474905265)\n",
      "('../data/documents\\\\ml002.pdf', 1.0245695126827221)\n",
      "The computing cost 0.011 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "cosine result\n",
      "('../data/documents\\\\ds002.pdf', 0.75765777)\n",
      "('../data/documents\\\\da001.pdf', 0.7421836)\n",
      "('../data/documents\\\\de004.pdf', 0.74125904)\n",
      "('../data/documents\\\\de003.pdf', 0.7226843)\n",
      "('../data/documents\\\\ds004.pdf', 0.70230144)\n",
      "The computing cost 4.812 seconds\n",
      "\n",
      "-------------------------------------------------------\n",
      "jacarrd result\n",
      "('../data/documents\\\\de003.pdf', 0.022727272727272728)\n",
      "('../data/documents\\\\da001.pdf', 0.022058823529411766)\n",
      "('../data/documents\\\\ml005.pdf', 0.020618556701030927)\n",
      "('../data/documents\\\\ml001.pdf', 0.0196078431372549)\n",
      "('../data/documents\\\\ds002.pdf', 0.018072289156626505)\n",
      "The computing cost 3.848 seconds\n",
      "\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"Enter search query\")\n",
    "keywords = input(\":: \")\n",
    "results = matching(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
